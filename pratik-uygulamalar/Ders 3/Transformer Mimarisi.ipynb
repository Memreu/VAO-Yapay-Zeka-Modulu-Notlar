{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda28546-df3f-498e-9e1a-9ee2214d94a3",
   "metadata": {},
   "source": [
    "##  Ders 3 Uygulamas覺 : Transformer Mimarisi ve 襤nteraktif eviri Sim羹lat繹r羹\n",
    "\n",
    "Bu proje, ChatGPT ve benzeri B羹y羹k Dil Modellerinin (LLM) temelini oluturan, 2017 y覺l覺nda literat羹re giren **Transformer** mimarisinin \"Tokenizasyon\" ve \"apraz Dikkat (Cross-Attention)\" aamalar覺n覺 interaktif bir sim羹lasyonla sunmaktad覺r.\n",
    "\n",
    "### Uygulaman覺n Amac覺 ve Yapay Zeka Felsefesi\n",
    "Toplumdaki genel kan覺n覺n aksine, doal dil ileme (NLP) algoritmalar覺 kelimeleri okumaz veya \"anlamaz\". Makine 繹renmesi modelleri i癟in metinler, uzaydaki matematiksel koordinatlar覺 temsil eden say覺lardan (vekt繹rlerden) ibarettir. Bu sim羹lasyon, makinenin insan dilini nas覺l say覺lara indirgediini ve bu say覺lar 羹zerinden nas覺l mant覺ksal eletirmeler yapt覺覺n覺 kan覺tlamak amac覺yla tasarlanm覺t覺r.\n",
    "\n",
    "### Sim羹lat繹r羹n al覺ma Mant覺覺 (Ad覺m Ad覺m Pipeline)\n",
    "\n",
    "Sim羹lat繹r羹 癟al覺t覺r覺p bir T羹rk癟e c羹mle girdiinizde, arka planda u m羹hendislik ad覺mlar覺 s覺ras覺yla ger癟ekleir:\n",
    "\n",
    "1. **Metin n 襤leme (Text Preprocessing):**\n",
    "   Kullan覺c覺n覺n girdii serbest metin, algoritmik hatalar覺 繹nlemek ad覺na standartlat覺r覺l覺r. B羹y羹k harfler k羹癟羹lt羹l羹r ve T羹rk癟e karakterler (癟, , 覺, 繹, , 羹) 襤ngilizce standartlar覺na d繹n羹t羹r羹l羹r.\n",
    "   \n",
    "2. **Tokenizasyon (Encoding):**\n",
    "   C羹mledeki her bir kelime, sistemin s繹zl羹羹nde (Vocabulary) yer alan benzersiz bir matematiksel kimlik numaras覺na (Token ID) d繹n羹t羹r羹l羹r. rnein, \"ben\" kelimesi makinenin zihninde `[101]` say覺s覺 olarak kodlan覺r. S繹zl羹kte olmayan kelimeler ise \"Bilinmeyen\" anlam覺na gelen `[999]` (UNK) ID'si ile etiketlenir.\n",
    "\n",
    "3. **apraz Dikkat ve Eletirme (Cross-Attention Mapping):**\n",
    "   Transformer mimarisinin temel mant覺覺d覺r. Sistem, elde ettii T羹rk癟e Token ID dizilimini al覺r ve 襤ngilizce dil uzay覺nda ayn覺 ID'ye sahip matematiksel kar覺l覺klar覺 bulur. Algoritma bu aamada kelimelerle deil, tamamen say覺larla (ID) ilem yapar.\n",
    "\n",
    "4. **襤nsan Diline D繹n羹 (Decoding):**\n",
    "   Elemesi tamamlanan ve hedef dile (襤ngilizceye) aktar覺lan say覺lar, \"Detokenization\" ilemi ile yeniden okunabilir metin format覺na 癟evrilerek kullan覺c覺ya nihai 癟覺kt覺 olarak sunulur.\n",
    "\n",
    "### 覺kar覺m\n",
    "Bu interaktif ara癟; \"Yapay zeka nas覺l d羹羹n羹r?\" sorusunun felsefi deil, tamamen matematiksel ve istatistiksel bir cevab覺d覺r. Kod 癟al覺t覺r覺ld覺覺nda, arka planda d繹nen say覺sal d繹n羹羹m matrisleri dorudan terminal ekran覺ndan izlenebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625f55c-547a-43e5-8d1c-a6bb81e97aba",
   "metadata": {},
   "source": [
    "### ! Not: Sim羹lasyon ve Ger癟ek D羹nya Sistemleri Aras覺ndaki Farklar\n",
    "\n",
    "Bu uygulamada gelitirilen \"Mini Transformer Sim羹lat繹r羹\", modern B羹y羹k Dil Modellerinin (LLM) kaputunun alt覺nda yatan veri ileme felsefesini (Tokenizasyon ve apraz Dikkat) kavramsal olarak kusursuz bir ekilde modeller. Ancak, bu kod mimarisi eitim ama癟l覺 tasarlanm覺 bir \"masa羹st羹 maketi\"dir. \n",
    "\n",
    "Ger癟ek hayatta end羹striyel 繹l癟ekte 癟al覺an sistemler (rn: GPT-4, Gemini) ile bu sim羹lasyon aras覺nda devasa matematiksel ve mimari farklar bulunur:\n",
    "\n",
    "* **Birebir Eleme (1:1 Mapping) vs. Balamsal Dikkat (Contextual Attention):**\n",
    "    Sim羹lat繹r羹m羹z kelimeleri sabit s繹zl羹klerle dorudan birbirine balar. Ger癟ek bir Transformer modeli ise kelimeleri sabit s繹zl羹klerle 癟evirmez; \"Sorgu, Anahtar ve Deer\" (Query, Key, Value) ad覺 verilen devasa matris 癟arp覺mlar覺 yaparak kelimenin o c羹mledeki \"balam覺na\" g繹re dinamik olarak karar verir.\n",
    "* **Kelime Bazl覺 vs. Alt-Kelime (BPE) Tokenizasyon:**\n",
    "    Sim羹lat繹r羹m羹z kelimeleri b羹t羹n olarak (boluklardan b繹lerek) okur ve s繹zl羹kte yoksa \"Bilinmeyen\" (UNK) hatas覺 verir. Ger癟ek sistemler ise kelimeleri hecelerine veya karakter gruplar覺na (Byte Pair Encoding - BPE) b繹ler. Bu sayede model, daha 繹nce hi癟 g繹rmedii bir kelimeyi bile par癟alar覺na ay覺rarak ileyebilir.\n",
    "* **Dorudan eviri vs. Diziden Diziye (Seq2Seq) retim:**\n",
    "    Sim羹lat繹r羹m羹z T羹rk癟e kelime s覺ras覺n覺 bozmadan dorudan 襤ngilizce kar覺l覺klar覺n覺 yan yana dizer. Ger癟ek modeller ise 癟覺kt覺 羹retirken (Decoding) hedef dilin gramer kurallar覺n覺 (rn: 襤ngilizcede y羹klemin ortada olmas覺) matematiksel olas覺l覺klarla hesaplay覺p kelimelerin yerlerini otomatik olarak yeniden d羹zenler.\n",
    "* **l癟ek ve Parametre Hacmi:**\n",
    "    Sim羹lat繹r羹m羹z yakla覺k 300 kelimelik bir s繹zl羹kle milisaniyeler i癟inde 癟al覺覺r. Modern dil modelleri ise 100.000'den fazla benzersiz token i癟eren s繹zl羹klere sahiptir ve milyarlarca parametre (a覺rl覺k) 羹zerinden devasa GPU k羹melerinde eitilir.\n",
    "\n",
    "**zetle:** Bu Sim羹lat繹r, devasa bir u癟a覺n 癟al覺ma prensibini g繹steren aerodinamik bir makettir. Temel fizik kurallar覺 (Tokenizasyon ve ID Eletirme) ger癟ektir, ancak motor g羹c羹 ve matematiksel karma覺kl覺k pekitirme ve eitim amac覺yla basitletirilmitir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee97fb8-5461-4745-a686-c0ce2458e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"INTERAKTIF TRANSFORMER: DIL ISLEME VE CEVIRI SIMULATORU \\n\")\n",
    "\n",
    "# --- 1. SOZLUK (VOCABULARY) VERITABANI (300+ Kelime) ---\n",
    "tr_vocab = {\n",
    "    # 1. Zamirler ve Soru Kelimeleri\n",
    "    \"ben\": 101, \"sen\": 102, \"o\": 103, \"biz\": 104, \"siz\": 105, \"onlar\": 106, \n",
    "    \"bu\": 107, \"su\": 108, \"bunlar\": 109, \"sunlar\": 110, \"hepsi\": 111, \"hicbiri\": 112, \n",
    "    \"kim\": 113, \"ne\": 114, \"nerede\": 115, \"nasil\": 116, \"neden\": 117, \"nicin\": 118, \n",
    "    \"ne_zaman\": 119, \"hangi\": 120, \"kac\": 121, \"kimin\": 122, \"nereye\": 123, \"nereden\": 124,\n",
    "\n",
    "    # 2. Teknoloji, Yapay Zeka ve Bilim\n",
    "    \"yapay\": 201, \"zeka\": 202, \"makine\": 203, \"ogrenmesi\": 204, \"dogal\": 205, \"dil\": 206, \n",
    "    \"veri\": 207, \"kod\": 208, \"python\": 209, \"model\": 210, \"matematik\": 211, \"algoritma\": 212, \n",
    "    \"bilgisayar\": 213, \"yazilim\": 214, \"donanim\": 215, \"ag\": 216, \"sistem\": 217, \"program\": 218, \n",
    "    \"dosya\": 219, \"ekran\": 220, \"klavye\": 221, \"fare\": 222, \"internet\": 223, \"teknoloji\": 224, \n",
    "    \"bilgi\": 225, \"islem\": 226, \"sunucu\": 227, \"bulut\": 228, \"robot\": 229, \"otonom\": 230, \n",
    "    \"derin\": 231, \"sinir\": 232, \"aglari\": 233, \"analiz\": 234, \"istatistik\": 235, \"bilim\": 236, \n",
    "    \"dijital\": 237, \"sanal\": 238, \"gerceklik\": 239,\n",
    "\n",
    "    # 3. Zaman ve Mekan\n",
    "    \"simdi\": 301, \"sonra\": 302, \"once\": 303, \"bugun\": 304, \"yarin\": 305, \"dun\": 306, \n",
    "    \"sabah\": 307, \"ogle\": 308, \"aksam\": 309, \"gece\": 310, \"burada\": 311, \"surada\": 312, \n",
    "    \"orada\": 313, \"iceri\": 314, \"disari\": 315, \"uzak\": 316, \"yakin\": 317, \"erken\": 318, \n",
    "    \"gec\": 319, \"her_zaman\": 320, \"bazen\": 321, \"asla\": 322, \"artik\": 323, \"henuz\": 324, \n",
    "    \"aniden\": 325, \"hemen\": 326, \"her_yer\": 327, \"hicbir_yer\": 328,\n",
    "\n",
    "    # 4. Isimler (Genel)\n",
    "    \"insan\": 401, \"adam\": 402, \"kadin\": 403, \"cocuk\": 404, \"aile\": 405, \"anne\": 406, \"baba\": 407, \n",
    "    \"arkadas\": 408, \"ev\": 409, \"okul\": 410, \"is\": 411, \"ofis\": 412, \"araba\": 413, \"yol\": 414, \n",
    "    \"sehir\": 415, \"ulke\": 416, \"dunya\": 417, \"hayat\": 418, \"zaman\": 419, \"gun\": 420, \"hafta\": 421, \n",
    "    \"ay\": 422, \"yil\": 423, \"su\": 424, \"yemek\": 425, \"kitap\": 426, \"masa\": 427, \"sandalye\": 428, \n",
    "    \"kalem\": 429, \"kapi\": 430, \"pencere\": 431, \"gunes\": 432, \"yildiz\": 433, \"hava\": 434, \n",
    "    \"deniz\": 435, \"agac\": 436, \"orman\": 437, \"hayvan\": 438, \"kedi\": 439, \"kopek\": 440, \"kus\": 441, \n",
    "    \"balik\": 442, \"para\": 443, \"sorun\": 444, \"cevap\": 445, \"isim\": 446, \"muzik\": 447, \"oyun\": 448, \n",
    "    \"film\": 449, \"resim\": 450, \"kalp\": 451, \"akil\": 452, \"ruh\": 453, \"ruya\": 454, \"gercek\": 455, \n",
    "    \"yalan\": 456, \"sarki\": 457, \"hikaye\": 458, \"tarih\": 459, \"sanat\": 460, \"toplum\": 461, \n",
    "    \"enerji\": 462, \"guc\": 463, \"fikir\": 464, \"duygu\": 465, \"umut\": 466, \"korku\": 467, \"sevgi\": 468, \n",
    "    \"baris\": 469, \"savas\": 470, \"basari\": 471, \"hata\": 472,\n",
    "\n",
    "    # 5. Fiiller (Simdiki Zaman - 1. Tekil Sahis & Kokler)\n",
    "    \"yapiyorum\": 501, \"ediyorum\": 502, \"gidiyorum\": 503, \"geliyorum\": 504, \"istiyorum\": 505, \n",
    "    \"biliyorum\": 506, \"dusunuyorum\": 507, \"anliyorum\": 508, \"calisiyorum\": 509, \"okuyorum\": 510, \n",
    "    \"yaziyorum\": 511, \"goruyorum\": 512, \"duyuyorum\": 513, \"izliyorum\": 514, \"seviyorum\": 515, \n",
    "    \"bekliyorum\": 516, \"soruyorum\": 517, \"ogreniyorum\": 518, \"kullaniyorum\": 519, \"konusuyorum\": 520, \n",
    "    \"dinliyorum\": 521, \"hissediyorum\": 522, \"basliyorum\": 523, \"bitiriyorum\": 524, \"aciyorum\": 525, \n",
    "    \"kapatiyorum\": 526, \"buluyorum\": 527, \"kaybediyorum\": 528, \"kazaniyorum\": 529, \"deniyorum\": 530, \n",
    "    \"cevapliyorum\": 531, \"ogretiyorum\": 532, \"yasiyorum\": 533, \"yiyorum\": 534, \"iciyorum\": 535, \n",
    "    \"uyuyorum\": 536, \"uyaniyorum\": 537, \"duruyorum\": 538, \"kosuyorum\": 539, \"yuruyorum\": 540, \n",
    "    \"oynuyorum\": 541, \"aliyorum\": 542, \"veriyorum\": 543, \"anlatiyorum\": 544, \"hatirliyorum\": 545, \n",
    "    \"unutuyorum\": 546, \"bakiyorum\": 547, \"inaniyorum\": 548, \"degisiyorum\": 549, \"uretiyorum\": 550,\n",
    "\n",
    "    # 6. Sifatlar ve Zarflar\n",
    "    \"iyi\": 601, \"kotu\": 602, \"guzel\": 603, \"cirkin\": 604, \"buyuk\": 605, \"kucuk\": 606, \n",
    "    \"uzun\": 607, \"kisa\": 608, \"sicak\": 609, \"soguk\": 610, \"yeni\": 611, \"eski\": 612, \n",
    "    \"genc\": 613, \"yasli\": 614, \"dogru\": 615, \"yanlis\": 616, \"hizli\": 617, \"yavas\": 618, \n",
    "    \"kolay\": 619, \"zor\": 620, \"ucuz\": 621, \"pahali\": 622, \"mutlu\": 623, \"uzgun\": 624, \n",
    "    \"guclu\": 625, \"zayif\": 626, \"temiz\": 627, \"kirli\": 628, \"dolu\": 629, \"bos\": 630, \n",
    "    \"acik\": 631, \"kapali\": 632, \"zengin\": 633, \"fakir\": 634, \"agir\": 635, \"hafif\": 636, \n",
    "    \"derin\": 637, \"sig\": 638, \"sert\": 639, \"yumusak\": 640, \"parlak\": 641, \"karanlik\": 642, \n",
    "    \"cesur\": 643, \"korkak\": 644, \"akilli\": 645, \"aptal\": 646, \"onemli\": 647, \"gereksiz\": 648, \n",
    "    \"farkli\": 649, \"ayni\": 650,\n",
    "\n",
    "    # 7. Renkler, Sayilar ve Miktarlar\n",
    "    \"kirmizi\": 701, \"mavi\": 702, \"yesil\": 703, \"sari\": 704, \"siyah\": 705, \"beyaz\": 706, \n",
    "    \"gri\": 707, \"mor\": 708, \"turuncu\": 709, \"kahverengi\": 710, \"pembe\": 711, \n",
    "    \"bir\": 712, \"iki\": 713, \"uc\": 714, \"dort\": 715, \"bes\": 716, \"alti\": 717, \"yedi\": 718, \n",
    "    \"sekiz\": 719, \"dokuz\": 720, \"on\": 721, \"yuz\": 722, \"bin\": 723, \n",
    "    \"cok\": 724, \"az\": 725, \"biraz\": 726, \"fazla\": 727, \"butun\": 728, \"bazi\": 729, \"hic\": 730,\n",
    "\n",
    "    # 8. Baglaclar ve Edatlar (Genel)\n",
    "    \"ve\": 801, \"veya\": 802, \"ama\": 803, \"fakat\": 804, \"icin\": 805, \"cunku\": 806, \n",
    "    \"ile\": 807, \"gibi\": 808, \"kadar\": 809, \"eger\": 810, \"ragmen\": 811, \n",
    "    \"merhaba\": 812, \"evet\": 813, \"hayir\": 814, \"lutfen\": 815, \"tesekkurler\": 816, \n",
    "    \"tamam\": 817, \"selam\": 818, \"hosca_kal\": 819, \"gunaydin\": 820, \"iyi_geceler\": 821, \n",
    "    \"belki\": 822, \"kesinlikle\": 823, \"genellikle\": 824, \"tabii_ki\": 825\n",
    "}\n",
    "\n",
    "en_vocab = {\n",
    "    # 1. Pronouns & Questions\n",
    "    101: \"I\", 102: \"you\", 103: \"he/she/it\", 104: \"we\", 105: \"you all\", 106: \"they\", \n",
    "    107: \"this\", 108: \"that\", 109: \"these\", 110: \"those\", 111: \"all\", 112: \"none\", \n",
    "    113: \"who\", 114: \"what\", 115: \"where\", 116: \"how\", 117: \"why\", 118: \"why\", \n",
    "    119: \"when\", 120: \"which\", 121: \"how many\", 122: \"whose\", 123: \"to where\", 124: \"from where\",\n",
    "\n",
    "    # 2. Tech & Science\n",
    "    201: \"artificial\", 202: \"intelligence\", 203: \"machine\", 204: \"learning\", 205: \"natural\", 206: \"language\", \n",
    "    207: \"data\", 208: \"code\", 209: \"python\", 210: \"model\", 211: \"math\", 212: \"algorithm\", \n",
    "    213: \"computer\", 214: \"software\", 215: \"hardware\", 216: \"network\", 217: \"system\", 218: \"program\", \n",
    "    219: \"file\", 220: \"screen\", 221: \"keyboard\", 222: \"mouse\", 223: \"internet\", 224: \"technology\", \n",
    "    225: \"information\", 226: \"process\", 227: \"server\", 228: \"cloud\", 229: \"robot\", 230: \"autonomous\", \n",
    "    231: \"deep\", 232: \"neural\", 233: \"networks\", 234: \"analysis\", 235: \"statistics\", 236: \"science\", \n",
    "    237: \"digital\", 238: \"virtual\", 239: \"reality\",\n",
    "\n",
    "    # 3. Time & Place\n",
    "    301: \"now\", 302: \"later\", 303: \"before\", 304: \"today\", 305: \"tomorrow\", 306: \"yesterday\", \n",
    "    307: \"morning\", 308: \"noon\", 309: \"evening\", 310: \"night\", 311: \"here\", 312: \"there\", \n",
    "    313: \"over there\", 314: \"inside\", 315: \"outside\", 316: \"far\", 317: \"near\", 318: \"early\", \n",
    "    319: \"late\", 320: \"always\", 321: \"sometimes\", 322: \"never\", 323: \"anymore\", 324: \"yet\", \n",
    "    325: \"suddenly\", 326: \"immediately\", 327: \"everywhere\", 328: \"nowhere\",\n",
    "\n",
    "    # 4. Nouns\n",
    "    401: \"human\", 402: \"man\", 403: \"woman\", 404: \"child\", 405: \"family\", 406: \"mother\", 407: \"father\", \n",
    "    408: \"friend\", 409: \"home\", 410: \"school\", 411: \"job\", 412: \"office\", 413: \"car\", 414: \"road\", \n",
    "    415: \"city\", 416: \"country\", 417: \"world\", 418: \"life\", 419: \"time\", 420: \"day\", 421: \"week\", \n",
    "    422: \"month\", 423: \"year\", 424: \"water\", 425: \"food\", 426: \"book\", 427: \"table\", 428: \"chair\", \n",
    "    429: \"pen\", 430: \"door\", 431: \"window\", 432: \"sun\", 433: \"star\", 434: \"weather\", \n",
    "    435: \"sea\", 436: \"tree\", 437: \"forest\", 438: \"animal\", 439: \"cat\", 440: \"dog\", 441: \"bird\", \n",
    "    442: \"fish\", 443: \"money\", 444: \"problem\", 445: \"answer\", 446: \"name\", 447: \"music\", 448: \"game\", \n",
    "    449: \"movie\", 450: \"picture\", 451: \"heart\", 452: \"mind\", 453: \"soul\", 454: \"dream\", 455: \"truth\", \n",
    "    456: \"lie\", 457: \"song\", 458: \"story\", 459: \"history\", 460: \"art\", 461: \"society\", \n",
    "    462: \"energy\", 463: \"power\", 464: \"idea\", 465: \"emotion\", 466: \"hope\", 467: \"fear\", 468: \"love\", \n",
    "    469: \"peace\", 470: \"war\", 471: \"success\", 472: \"mistake\",\n",
    "\n",
    "    # 5. Verbs (Continuous/Base matching)\n",
    "    501: \"am doing\", 502: \"am making\", 503: \"am going\", 504: \"am coming\", 505: \"want\", \n",
    "    506: \"know\", 507: \"am thinking\", 508: \"understand\", 509: \"am working\", 510: \"am reading\", \n",
    "    511: \"am writing\", 512: \"see\", 513: \"hear\", 514: \"am watching\", 515: \"love\", \n",
    "    516: \"am waiting\", 517: \"am asking\", 518: \"am learning\", 519: \"am using\", 520: \"am speaking\", \n",
    "    521: \"am listening\", 522: \"feel\", 523: \"am starting\", 524: \"am finishing\", 525: \"am opening\", \n",
    "    526: \"am closing\", 527: \"am finding\", 528: \"am losing\", 529: \"am winning\", 530: \"am trying\", \n",
    "    531: \"am answering\", 532: \"am teaching\", 533: \"am living\", 534: \"am eating\", 535: \"am drinking\", \n",
    "    536: \"am sleeping\", 537: \"am waking up\", 538: \"am standing\", 539: \"am running\", 540: \"am walking\", \n",
    "    541: \"am playing\", 542: \"am taking\", 543: \"am giving\", 544: \"am explaining\", 545: \"remember\", \n",
    "    546: \"forget\", 547: \"am looking\", 548: \"believe\", 549: \"am changing\", 550: \"am producing\",\n",
    "\n",
    "    # 6. Adjectives & Adverbs\n",
    "    601: \"good\", 602: \"bad\", 603: \"beautiful\", 604: \"ugly\", 605: \"big\", 606: \"small\", \n",
    "    607: \"long\", 608: \"short\", 609: \"hot\", 610: \"cold\", 611: \"new\", 612: \"old\", \n",
    "    613: \"young\", 614: \"old\", 615: \"right\", 616: \"wrong\", 617: \"fast\", 618: \"slow\", \n",
    "    619: \"easy\", 620: \"hard\", 621: \"cheap\", 622: \"expensive\", 623: \"happy\", 624: \"sad\", \n",
    "    625: \"strong\", 626: \"weak\", 627: \"clean\", 628: \"dirty\", 629: \"full\", 630: \"empty\", \n",
    "    631: \"open\", 632: \"closed\", 633: \"rich\", 634: \"poor\", 635: \"heavy\", 636: \"light\", \n",
    "    637: \"deep\", 638: \"shallow\", 639: \"hard\", 640: \"soft\", 641: \"bright\", 642: \"dark\", \n",
    "    643: \"brave\", 644: \"coward\", 645: \"smart\", 646: \"stupid\", 647: \"important\", 648: \"unnecessary\", \n",
    "    649: \"different\", 650: \"same\",\n",
    "\n",
    "    # 7. Colors, Numbers & Quantifiers\n",
    "    701: \"red\", 702: \"blue\", 703: \"green\", 704: \"yellow\", 705: \"black\", 706: \"white\", \n",
    "    707: \"gray\", 708: \"purple\", 709: \"orange\", 710: \"brown\", 711: \"pink\", \n",
    "    712: \"one\", 713: \"two\", 714: \"three\", 715: \"four\", 716: \"five\", 717: \"six\", 718: \"seven\", \n",
    "    719: \"eight\", 720: \"nine\", 721: \"ten\", 722: \"hundred\", 723: \"thousand\", \n",
    "    724: \"very/a lot\", 725: \"little\", 726: \"some\", 727: \"too much\", 728: \"whole\", 729: \"some\", 730: \"none\",\n",
    "\n",
    "    # 8. Conjunctions & Particles\n",
    "    801: \"and\", 802: \"or\", 803: \"but\", 804: \"however\", 805: \"for\", 806: \"because\", \n",
    "    807: \"with\", 808: \"like\", 809: \"until/as much as\", 810: \"if\", 811: \"despite\", \n",
    "    812: \"hello\", 813: \"yes\", 814: \"no\", 815: \"please\", 816: \"thanks\", \n",
    "    817: \"okay\", 818: \"hi\", 819: \"goodbye\", 820: \"good morning\", 821: \"good night\", \n",
    "    822: \"maybe\", 823: \"definitely\", 824: \"usually\", 825: \"of course\"\n",
    "}\n",
    "\n",
    "def metni_temizle(metin):\n",
    "    degisimler = str.maketrans(\"癟覺繹羹襤\", \"cgiosuCGIOSU\")\n",
    "    return metin.translate(degisimler).lower()\n",
    "\n",
    "def tokenize_et(metin):\n",
    "    temiz_metin = metni_temizle(metin)\n",
    "    kelimeler = temiz_metin.split()\n",
    "    tokens = []\n",
    "    for kelime in kelimeler:\n",
    "        token_id = tr_vocab.get(kelime, 999) \n",
    "        tokens.append((kelime, token_id))\n",
    "    return tokens\n",
    "\n",
    "def gorsellestir(tr_tokens, en_kelimeler):\n",
    "    boyut = max(4, len(tr_tokens) * 1.2)\n",
    "    fig, ax = plt.subplots(figsize=(10, boyut))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.title(\"Capraz Dikkat (Cross-Attention) ve Token Eslesmesi\", fontsize=14, pad=20)\n",
    "    \n",
    "    # Basliklar\n",
    "    ax.text(0.15, len(tr_tokens) + 0.5, \"Turkce Girdi\", ha='center', fontweight='bold', fontsize=11)\n",
    "    ax.text(0.50, len(tr_tokens) + 0.5, \"Token ID (Makine Dili)\", ha='center', fontweight='bold', fontsize=11)\n",
    "    ax.text(0.85, len(tr_tokens) + 0.5, \"Ingilizce Cikti\", ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "    for i, (tr_word, t_id) in enumerate(tr_tokens):\n",
    "        y = len(tr_tokens) - i \n",
    "        en_word = en_kelimeler[i]\n",
    "        \n",
    "        # Kutular\n",
    "        ax.text(0.15, y, tr_word, ha='center', va='center', fontsize=12, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"#e0f7fa\", edgecolor=\"#00bcd4\"))\n",
    "        \n",
    "        ax.text(0.50, y, f\"[{t_id}]\", ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "                bbox=dict(boxstyle=\"square,pad=0.4\", facecolor=\"#fff9c4\", edgecolor=\"#fbc02d\"))\n",
    "        \n",
    "        ax.text(0.85, y, en_word, ha='center', va='center', fontsize=12, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"#e8f5e9\", edgecolor=\"#4caf50\"))\n",
    "        \n",
    "        # Oklari Ciz (TR -> ID -> EN)\n",
    "        ax.annotate(\"\", xy=(0.42, y), xytext=(0.25, y), arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=1.5))\n",
    "        ax.annotate(\"\", xy=(0.75, y), xytext=(0.58, y), arrowprops=dict(arrowstyle=\"->\", color=\"gray\", lw=1.5))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- 2. KULLANICI ETKILEISIMI ---\n",
    "print(\"Sistem Hazir. Genisletilmis sozluk devrede.\")\n",
    "print(\"Ornek kelimeler: ben, simdi, kod, yaziyorum, dunya, nerede, neden, anliyorum...\\n\")\n",
    "kullanici_metni = input(\"Lutfen cevrilecek Turkce bir cumle girin: \")\n",
    "\n",
    "if not kullanici_metni.strip():\n",
    "    kullanici_metni = \"ben python ogreniyorum ve makine anliyorum\"\n",
    "    print(f\"\\nBos giris yapildi. Varsayilan test cumlesi kullaniliyor: '{kullanici_metni}'\")\n",
    "\n",
    "# --- 3. ASAMA: TOKENIZASYON VE CEVIRI ISLEMLERI ---\n",
    "print(\"\\nASAMA 1: TOKENIZASYON (Makinenin Anlama Sureci)\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "tr_tokens = tokenize_et(kullanici_metni)\n",
    "girdi_vektoru = [t[1] for t in tr_tokens]\n",
    "\n",
    "print(\"Yapay zeka cumleyi parcaliyor ve kelimeleri Token ID'lerine donusturuyor:\")\n",
    "for kelime, token_id in tr_tokens:\n",
    "    durum = \"(Bilinmeyen Kelime - UNK)\" if token_id == 999 else \"\"\n",
    "    print(f\"   Kelime: '{kelime:<12}' -> Token ID: [{token_id}] {durum}\")\n",
    "\n",
    "print(f\"\\nSistemin Isleme Aldigi Saf Vektor Dizilimi: {girdi_vektoru}\")\n",
    "\n",
    "print(\"\\nASAMA 2: CAPRAZ DIKKAT VE ESLESTIRME (Cross-Attention)\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "print(\"Sistem sayi dizilimini alip Ingilizce uzayindaki karsiliklariyla eslestiriyor. (Gorsel aciliyor...)\")\n",
    "\n",
    "ingilizce_cumle = []\n",
    "for kelime, token_id in tr_tokens:\n",
    "    if token_id == 999:\n",
    "        ingilizce_cumle.append(\"[UNKNOWN]\")\n",
    "    else:\n",
    "        ingilizce_cumle.append(en_vocab[token_id])\n",
    "\n",
    "# Gorsellestirme fonksiyonunu cagiriyoruz\n",
    "gorsellestir(tr_tokens, ingilizce_cumle)\n",
    "\n",
    "print(\"\\nASAMA 3: INSAN DILINE DONUS (Decoding)\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "final_metin = \" \".join(ingilizce_cumle)\n",
    "print(\"Bulunan Ingilizce tokenlar tekrar anlamli bir metne birlestiriliyor...\\n\")\n",
    "print(f\"CEVIRI SONUCU: '{final_metin.capitalize()}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172ce31-62ee-4bd6-bb3d-8d4cf92d9188",
   "metadata": {},
   "source": [
    "### Kendi Kelimelerini Ekle: S繹zl羹k (Vocabulary) Nas覺l Geniletilir?\n",
    "\n",
    "Sistemin 癟eviri yeteneini kendi c羹mlelerinize g繹re uyarlamak 癟ok kolayd覺r. Yeni kelime eklerken u iki kurala dikkat etmeniz yeterlidir:\n",
    "\n",
    "1. **Ayn覺 ID Numaras覺n覺 Kullan覺n:** Makine kelimeleri deil, say覺lar覺 (Token ID) eletirir. T羹rk癟e ve 襤ngilizce s繹zl羹e eklediiniz kelimelerin kimlik numaras覺 birebir ayn覺 olmal覺d覺r.\n",
    "2. **T羹rk癟e Karakter Kullanmay覺n:** Kodun veriyi hatas覺z okuyabilmesi i癟in `tr_vocab` s繹zl羹羹ne kelime eklerken tamamen k羹癟羹k harf kullan覺n ve T羹rk癟e karakter (癟, , 覺, 繹, , 羹) **i癟ermemesine** dikkat edin. (襤癟erse bile sorun 癟覺kmamas覺 i癟in d羹zenleme yap覺ld覺 yine de gelecekteki projeler i癟in dikkat edilmeli)\n",
    "\n",
    "**rnek Ekleme:**\n",
    "Diyelim ki sisteme \"m羹hendis\" kelimesini eklemek istiyoruz ve bota olan `901` ID'sini se癟tik:\n",
    "\n",
    "```python\n",
    "# 1. T羹rk癟e s繹zl羹e (tr_vocab) ekle:\n",
    "\"muhendis\": 901,  # '羹' harfi yerine 'u' kullan覺ld覺\n",
    "\n",
    "# 2. 襤ngilizce s繹zl羹e (en_vocab) AYNI ID ile ekle:\n",
    "901: \"engineer\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38866a0c-672f-4066-8209-eac9a1c9b8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
